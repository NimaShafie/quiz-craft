services:
  streamlit:
    build: .  # Build from the local Dockerfile
    container_name: quiz_craft_app_streamlit  # Name the container for easier referencing
    ports:
      - "8501:8501"  # Expose Streamlit port
    environment:
      - OLLAMA_HOST=http://ollama:11434  # Point Streamlit to the Ollama host
    depends_on:
      - ollama  # Ensure Ollama starts first
    restart: unless-stopped
    networks:
      - app_network

  ollama:
    image: ollama/ollama  # Use the official Ollama image
    container_name: quiz_craft_llm_ollama  # Name the container for easier referencing
    command: ["serve"]  # start with serve (only accepts this command)
    ports:
      - "11434:11434"  # Expose Ollama's API port
    volumes:
      - ollama_models:/root/.ollama/models  # Persist Ollama's models
    restart: no
    networks:
      - app_network

volumes:
  ollama_models:  # Named volume to store Ollama's models

networks:
  app_network:
    driver: bridge
