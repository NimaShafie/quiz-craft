# WIP
# needs updates and finalization

# build dockerfile (or update it)
docker build -t quiz-craft .

# docker compose build (build with a Dockerfile + docker-compose.yml)
docker-compose up

docker run -d -p 8501:8501 quiz-craft
ollama run llama3:latest

--------------

trying this model

ollama llama3
https://ollama.com/library/llama3

8b, 4.7gb
# 365c0bd3c000

# uninstall all pip packages
pip freeze > deletelist.txt && pip uninstall -y -r deletelist.txt

# create a pip requirements from based on your packages
pip freeze > requirements.txt

# how to start ollama
ollama run llama3:latest
Ctrl + d to exit the prompt window


working Dockerfile
# Use an official Python runtime as a parent image
FROM python:3.12-slim

# Set the working directory in the container
WORKDIR /app

# Copy all necessary files and folders into the container
COPY . /app

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# # Set the Python entry point directory as src/
# WORKDIR /app/src

# Ensure required files are available in the correct paths
RUN test -f /app/response.json || echo "{}" > /app/response.json

# Install dependencies for downloading and installing Ollama CLI
RUN apt-get update && apt-get install -y \
    curl \
    tar \
    libstdc++6 \
    procps \
    && apt-get clean

# Define Ollama CLI URL as an environment variable
ENV OLLAMA_CLI_URL=https://ollama.com/download/linux

# Install Ollama CLI
RUN curl -sSfL https://ollama.com/install.sh -o /tmp/ollama_installer.sh && \
    chmod +x /tmp/ollama_installer.sh && \
    sh /tmp/ollama_installer.sh

# Start Ollama serve, verify version, and then stop it
RUN ollama serve & \
    sleep 5 && \
    ollama --version && \
    pkill -f "ollama serve"

# Expose application and Ollama ports
EXPOSE 80
EXPOSE 11434

# Expose the Streamlit default port
EXPOSE 8501

# Define environment variable
ENV NAME=World

# Run the Python app using Streamlit
CMD ["streamlit", "run", "/src/QuizCraft.py"]



#!/bin/sh
ollama serve &
sleep 5  # Give Ollama some time to start
ollama pull llama3
ollama run llama3:latest



----
this will prob work
Steps to Run the Setup
Pull the Required Images and Build Streamlit:

bash
Copy code
docker-compose build
docker-compose pull ollama
Start the Containers:

bash
Copy code
docker-compose up -d
Run the llama3 Model in the Ollama Container: To run the llama3 model locally within the ollama container:

bash
Copy code
docker exec -it ollama ollama run llama3
This command starts the llama3 model in the ollama container. It will listen for requests from the Streamlit app at OLLAMA_HOST=http://ollama:11434.











--- it works using this method (older)
run the separate "ollama" container (image: ollama/ollama)

then docker-compose up -d, then stop the failed ollama container (that gets spawned with streamlit)
then continue with: localhost:8501 (to check if streamlit works)
then continue to check: localhost:11434 (to check if ollama works)


--- newest working method (12/4/2024)
run: docker-compose up --build -d
